{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- 1. LightGBM 최적 파라미터로 모델 학습 --------------------------\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 190123, number of negative: 190123\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 11627\n",
      "[LightGBM] [Info] Number of data points in the train set: 380246, number of used features: 55\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 2\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 17 dense feature groups (7.25 MB) transferred to GPU in 0.005510 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "LightGBM 모델의 ROC-AUC 점수: 0.9123\n",
      "-------------------------- 2. XGBoost 최적 파라미터로 모델 학습 --------------------------\n",
      "XGBoost 모델의 ROC-AUC 점수: 0.9323\n",
      "-------------------------- 3. CatBoost 최적 파라미터로 모델 학습 --------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 9.41ms\tremaining: 9.4s\n",
      "100:\ttotal: 766ms\tremaining: 6.82s\n",
      "200:\ttotal: 1.5s\tremaining: 5.97s\n",
      "300:\ttotal: 2.26s\tremaining: 5.25s\n",
      "400:\ttotal: 3.02s\tremaining: 4.51s\n",
      "500:\ttotal: 3.77s\tremaining: 3.75s\n",
      "600:\ttotal: 4.52s\tremaining: 3s\n",
      "700:\ttotal: 5.28s\tremaining: 2.25s\n",
      "800:\ttotal: 6.04s\tremaining: 1.5s\n",
      "900:\ttotal: 6.8s\tremaining: 747ms\n",
      "999:\ttotal: 7.56s\tremaining: 0us\n",
      "CatBoost 모델의 ROC-AUC 점수: 0.9090\n",
      "-------------------------- 6. 최종 모델 학습 및 제출 파일 생성 --------------------------\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 190123, number of negative: 190123\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 11627\n",
      "[LightGBM] [Info] Number of data points in the train set: 380246, number of used features: 55\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 2\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 17 dense feature groups (7.25 MB) transferred to GPU in 0.007265 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 9.15ms\tremaining: 9.14s\n",
      "100:\ttotal: 755ms\tremaining: 6.72s\n",
      "200:\ttotal: 1.5s\tremaining: 5.97s\n",
      "300:\ttotal: 2.23s\tremaining: 5.19s\n",
      "400:\ttotal: 2.95s\tremaining: 4.41s\n",
      "500:\ttotal: 3.65s\tremaining: 3.64s\n",
      "600:\ttotal: 4.36s\tremaining: 2.9s\n",
      "700:\ttotal: 5.08s\tremaining: 2.17s\n",
      "800:\ttotal: 5.79s\tremaining: 1.44s\n",
      "900:\ttotal: 6.5s\tremaining: 715ms\n",
      "999:\ttotal: 7.22s\tremaining: 0us\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 126749, number of negative: 126748\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 11949\n",
      "[LightGBM] [Info] Number of data points in the train set: 253497, number of used features: 55\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 2\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 17 dense feature groups (4.84 MB) transferred to GPU in 0.004350 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500002 -> initscore=0.000008\n",
      "[LightGBM] [Info] Start training from score 0.000008\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 126748, number of negative: 126749\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 11000\n",
      "[LightGBM] [Info] Number of data points in the train set: 253497, number of used features: 55\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 2\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 17 dense feature groups (4.84 MB) transferred to GPU in 0.004014 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499998 -> initscore=-0.000008\n",
      "[LightGBM] [Info] Start training from score -0.000008\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 126749, number of negative: 126749\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 10992\n",
      "[LightGBM] [Info] Number of data points in the train set: 253498, number of used features: 55\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 2\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 16 dense feature groups (3.87 MB) transferred to GPU in 0.003567 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 8.06ms\tremaining: 8.05s\n",
      "100:\ttotal: 678ms\tremaining: 6.03s\n",
      "200:\ttotal: 1.32s\tremaining: 5.25s\n",
      "300:\ttotal: 1.97s\tremaining: 4.57s\n",
      "400:\ttotal: 2.61s\tremaining: 3.9s\n",
      "500:\ttotal: 3.24s\tremaining: 3.23s\n",
      "600:\ttotal: 3.87s\tremaining: 2.57s\n",
      "700:\ttotal: 4.5s\tremaining: 1.92s\n",
      "800:\ttotal: 5.14s\tremaining: 1.28s\n",
      "900:\ttotal: 5.76s\tremaining: 633ms\n",
      "999:\ttotal: 6.39s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 7.44ms\tremaining: 7.43s\n",
      "100:\ttotal: 674ms\tremaining: 6s\n",
      "200:\ttotal: 1.34s\tremaining: 5.31s\n",
      "300:\ttotal: 2.01s\tremaining: 4.67s\n",
      "400:\ttotal: 2.69s\tremaining: 4.01s\n",
      "500:\ttotal: 3.36s\tremaining: 3.35s\n",
      "600:\ttotal: 4.04s\tremaining: 2.68s\n",
      "700:\ttotal: 4.74s\tremaining: 2.02s\n",
      "800:\ttotal: 5.44s\tremaining: 1.35s\n",
      "900:\ttotal: 6.15s\tremaining: 675ms\n",
      "999:\ttotal: 6.85s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 7.6ms\tremaining: 7.59s\n",
      "100:\ttotal: 688ms\tremaining: 6.12s\n",
      "200:\ttotal: 1.37s\tremaining: 5.45s\n",
      "300:\ttotal: 2.06s\tremaining: 4.78s\n",
      "400:\ttotal: 2.74s\tremaining: 4.1s\n",
      "500:\ttotal: 3.4s\tremaining: 3.39s\n",
      "600:\ttotal: 4.1s\tremaining: 2.72s\n",
      "700:\ttotal: 4.77s\tremaining: 2.03s\n",
      "800:\ttotal: 5.44s\tremaining: 1.35s\n",
      "900:\ttotal: 6.14s\tremaining: 675ms\n",
      "999:\ttotal: 6.84s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/najo/.conda/envs/dip/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost 모델의 ROC-AUC 점수: 0.9284\n",
      "최종 제출 파일 'stacking_ensemble_submit.csv' 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import  OrdinalEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "numeric_columns = [\n",
    "    \"임신 시도 또는 마지막 임신 경과 연수\",\n",
    "    \"총 생성 배아 수\",\n",
    "    \"미세주입된 난자 수\",\n",
    "    \"미세주입에서 생성된 배아 수\",\n",
    "    \"이식된 배아 수\",\n",
    "    \"미세주입 배아 이식 수\",\n",
    "    \"저장된 배아 수\",\n",
    "    \"미세주입 후 저장된 배아 수\",\n",
    "    \"해동된 배아 수\",\n",
    "    \"해동 난자 수\",\n",
    "    \"수집된 신선 난자 수\",\n",
    "    \"저장된 신선 난자 수\",\n",
    "    \"혼합된 난자 수\",\n",
    "    \"파트너 정자와 혼합된 난자 수\",\n",
    "    \"기증자 정자와 혼합된 난자 수\",\n",
    "    \"난자 채취 경과일\",\n",
    "    \"난자 해동 경과일\",\n",
    "    \"난자 혼합 경과일\",\n",
    "    \"배아 이식 경과일\",\n",
    "    \"배아 해동 경과일\"\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    \"시술 시기 코드\",\n",
    "    \"시술 당시 나이\",\n",
    "    \"시술 유형\",\n",
    "    \"특정 시술 유형\",\n",
    "    \"배란 자극 여부\",\n",
    "    \"배란 유도 유형\",\n",
    "    \"단일 배아 이식 여부\",\n",
    "    \"착상 전 유전 검사 사용 여부\",\n",
    "    \"착상 전 유전 진단 사용 여부\",\n",
    "    \"남성 주 불임 원인\",\n",
    "    \"남성 부 불임 원인\",\n",
    "    \"여성 주 불임 원인\",\n",
    "    \"여성 부 불임 원인\",\n",
    "    \"부부 주 불임 원인\",\n",
    "    \"부부 부 불임 원인\",\n",
    "    \"불명확 불임 원인\",\n",
    "    \"불임 원인 - 난관 질환\",\n",
    "    \"불임 원인 - 남성 요인\",\n",
    "    \"불임 원인 - 배란 장애\",\n",
    "    \"불임 원인 - 여성 요인\",\n",
    "    \"불임 원인 - 자궁경부 문제\",\n",
    "    \"불임 원인 - 자궁내막증\",\n",
    "    \"불임 원인 - 정자 농도\",\n",
    "    \"불임 원인 - 정자 면역학적 요인\",\n",
    "    \"불임 원인 - 정자 운동성\",\n",
    "    \"불임 원인 - 정자 형태\",\n",
    "    \"배아 생성 주요 이유\",\n",
    "    \"총 시술 횟수\",\n",
    "    \"클리닉 내 총 시술 횟수\",\n",
    "    \"IVF 시술 횟수\",\n",
    "    \"DI 시술 횟수\",\n",
    "    \"총 임신 횟수\",\n",
    "    \"IVF 임신 횟수\",\n",
    "    \"DI 임신 횟수\",\n",
    "    \"총 출산 횟수\",\n",
    "    \"IVF 출산 횟수\",\n",
    "    \"DI 출산 횟수\",\n",
    "    \"난자 출처\",\n",
    "    \"정자 출처\",\n",
    "    \"난자 기증자 나이\",\n",
    "    \"정자 기증자 나이\",\n",
    "    \"동결 배아 사용 여부\",\n",
    "    \"신선 배아 사용 여부\",\n",
    "    \"기증 배아 사용 여부\",\n",
    "    \"대리모 여부\",\n",
    "    \"PGD 시술 여부\",\n",
    "    \"PGS 시술 여부\"\n",
    "]\n",
    "\n",
    "# === 이상치 대체 (Winsorizing) 함수 정의 ===\n",
    "def winsorize(df, numeric_cols, factor=1.5):\n",
    "    df_new = df.copy()\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df_new[col].quantile(0.25)\n",
    "        Q3 = df_new[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - factor * IQR\n",
    "        upper_bound = Q3 + factor * IQR\n",
    "        df_new[col] = df_new[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    return df_new\n",
    "\n",
    "train = pd.read_csv('./data/train.csv').drop(columns=['ID'])\n",
    "test = pd.read_csv('./data/test.csv').drop(columns=['ID'])\n",
    "\n",
    "train = winsorize(train, numeric_columns, factor=1.5)\n",
    "test = winsorize(test, numeric_columns, factor=1.5)\n",
    "\n",
    "X = train.drop('임신 성공 여부', axis=1)\n",
    "y = train['임신 성공 여부']\n",
    "\n",
    "# 카테고리형 컬럼들을 문자열로 변환\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].astype(str)\n",
    "    test[col] = test[col].astype(str)\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "X_train_encoded = X.copy()\n",
    "X_train_encoded[categorical_columns] = ordinal_encoder.fit_transform(X[categorical_columns])\n",
    "\n",
    "X_test_encoded = test.copy()\n",
    "X_test_encoded[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])\n",
    "\n",
    "# 수치형 컬럼들을 0으로 채움\n",
    "X_train_encoded[numeric_columns] = X_train_encoded[numeric_columns].fillna(0)\n",
    "X_test_encoded[numeric_columns] = X_test_encoded[numeric_columns].fillna(0)\n",
    "\n",
    "# categorical_feature에 인덱스를 전달\n",
    "categorical_feature_indices = [X_train_encoded.columns.get_loc(col) for col in categorical_columns]\n",
    "\n",
    "# 스케일링 적용\n",
    "scaler = StandardScaler()\n",
    "X_train_encoded[numeric_columns] = scaler.fit_transform(X_train_encoded[numeric_columns])\n",
    "X_test_encoded[numeric_columns] = scaler.transform(X_test_encoded[numeric_columns])\n",
    "\n",
    "# 데이터 불균형 확인 및 SMOTE 적용\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_encoded, y = smote.fit_resample(X_train_encoded, y)\n",
    "\n",
    "# 데이터 불균형 확인 및 SMOTE + Tomek 적용\n",
    "# smt = SMOTETomek(sampling_strategy=0.8, random_state=42)\n",
    "# X_train_encoded, y = smt.fit_resample(X_train_encoded, y)\n",
    "\n",
    "# # Feature Selection (SelectKBest)\n",
    "# selector = SelectKBest(score_func=mutual_info_classif, k=40)  # 가장 중요한 40개의 변수를 선택\n",
    "# X_train_encoded = selector.fit_transform(X_train_encoded, y)\n",
    "# X_test_encoded = selector.transform(X_test_encoded)\n",
    "\n",
    "print(\"-------------------------- 1. LightGBM 최적 파라미터로 모델 학습 --------------------------\")\n",
    "lgb_est = LGBMClassifier(\n",
    "    random_state=42,\n",
    "    objective='binary',\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=70,\n",
    "    max_depth=-1,\n",
    "    device='gpu',  # GPU 사용\n",
    "    gpu_platform_id=0,\n",
    "    gpu_device_id=2\n",
    ")\n",
    "\n",
    "# LightGBM 모델 학습\n",
    "lgb_est.fit(X_train_encoded, y)\n",
    "y_train_pred_lgb = lgb_est.predict_proba(X_train_encoded)[:, 1]\n",
    "roc_auc_lgb = roc_auc_score(y, y_train_pred_lgb)\n",
    "print(f\"LightGBM 모델의 ROC-AUC 점수: {roc_auc_lgb:.4f}\")\n",
    "\n",
    "print(\"-------------------------- 2. XGBoost 최적 파라미터로 모델 학습 --------------------------\")\n",
    "xgb_est = XGBClassifier(\n",
    "    eval_metric='auc',\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=11,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.7,\n",
    "    gamma=0,\n",
    "    tree_method='hist',  # 새 권장 방식\n",
    "    device='cuda'        # GPU 사용\n",
    ")\n",
    "\n",
    "\n",
    "# XGBoost 모델 학습\n",
    "xgb_est.fit(X_train_encoded, y)\n",
    "y_train_pred_xgb = xgb_est.predict_proba(X_train_encoded)[:, 1]\n",
    "roc_auc_xgb = roc_auc_score(y, y_train_pred_xgb)\n",
    "print(f\"XGBoost 모델의 ROC-AUC 점수: {roc_auc_xgb:.4f}\")\n",
    "\n",
    "print(\"-------------------------- 3. CatBoost 최적 파라미터로 모델 학습 --------------------------\")\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.01,\n",
    "    depth=8,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    random_seed=42,\n",
    "    verbose=100,  # 경고 메시지 최소화\n",
    "    task_type='GPU',\n",
    "    devices='1'\n",
    ")\n",
    "\n",
    "\n",
    "# CatBoost 모델 학습\n",
    "catboost_model.fit(X_train_encoded, y)\n",
    "y_train_pred_cat = catboost_model.predict_proba(X_train_encoded)[:, 1]\n",
    "roc_auc_cat = roc_auc_score(y, y_train_pred_cat)\n",
    "print(f\"CatBoost 모델의 ROC-AUC 점수: {roc_auc_cat:.4f}\")\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lgb', lgb_est),\n",
    "        ('xgb', xgb_est),\n",
    "        ('cat', catboost_model)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(C=0.5, penalty='l2', solver='saga', max_iter=500, random_state=42),\n",
    "    cv=3,\n",
    "    n_jobs=1,\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "print(\"-------------------------- 6. 최종 모델 학습 및 제출 파일 생성 --------------------------\")\n",
    "\n",
    "# 테스트 데이터에 대한 예측 (Stacking 앙상블)\n",
    "stacking_clf.fit(X_train_encoded, y)\n",
    "y_test_pred_stacking = stacking_clf.predict_proba(X_train_encoded)[:, 1]\n",
    "ROC_AUC_stacking = roc_auc_score(y, y_test_pred_stacking)\n",
    "print(f\"CatBoost 모델의 ROC-AUC 점수: {ROC_AUC_stacking:.4f}\")\n",
    "final_pred_proba = stacking_clf.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "# 제출 파일 생성\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')\n",
    "sample_submission['probability'] = final_pred_proba\n",
    "sample_submission.to_csv('./submit/stacking_ensemble_submit.csv', index=False)\n",
    "print(\"최종 제출 파일 'stacking_ensemble_submit.csv' 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 교차 검증 설정\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv_scores_lgb = []\n",
    "# cv_scores_xgb = []\n",
    "# cv_scores_stack = []\n",
    "# cv_scores_catb = []\n",
    "\n",
    "# # 교차 검증을 통한 ROC-AUC 비교\n",
    "# for train_idx, val_idx in cv.split(X_train_encoded, y):\n",
    "#     X_cv_train, X_cv_val = X_train_encoded.iloc[train_idx], X_train_encoded.iloc[val_idx]\n",
    "#     y_cv_train, y_cv_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "#     # LightGBM 예측\n",
    "#     lgb_est.fit(X_cv_train, y_cv_train)\n",
    "#     lgb_proba = lgb_est.predict_proba(X_cv_val)[:, 1]\n",
    "#     auc_lgb = roc_auc_score(y_cv_val, lgb_proba)\n",
    "#     cv_scores_lgb.append(auc_lgb)\n",
    "    \n",
    "#     # XGBoost 예측\n",
    "#     xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "#     xgb_model.fit(X_cv_train, y_cv_train)\n",
    "#     xgb_proba = xgb_model.predict_proba(X_cv_val)[:, 1]\n",
    "#     auc_xgb = roc_auc_score(y_cv_val, xgb_proba)\n",
    "#     cv_scores_xgb.append(auc_xgb)\n",
    "\n",
    "#     # catboost 예측\n",
    "#     catboost_model.fit(X_cv_train, y_cv_train)\n",
    "#     cat_proba = catboost_model.predict_proba(X_cv_val)[:, 1]\n",
    "#     auc_catb = roc_auc_score(y_cv_val, cat_proba)\n",
    "#     cv_scores_catb.append(auc_catb)\n",
    "    \n",
    "#     # Stacking 앙상블 예측\n",
    "#     stacking_clf.fit(X_cv_train, y_cv_train)\n",
    "#     stack_proba = stacking_clf.predict_proba(X_cv_val)[:, 1]\n",
    "#     auc_stack = roc_auc_score(y_cv_val, stack_proba)\n",
    "#     cv_scores_stack.append(auc_stack)\n",
    "\n",
    "# # 평균 ROC-AUC 출력\n",
    "# print(\"------------- 교차 검증 평균 ROC-AUC -------------\")\n",
    "# print(f\"LightGBM: {np.mean(cv_scores_lgb):.4f}\")\n",
    "# print(f\"XGBoost: {np.mean(cv_scores_xgb):.4f}\")\n",
    "# print(f\"Stacking Ensemble: {np.mean(cv_scores_stack):.4f}\")\n",
    "# print(\"---------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
