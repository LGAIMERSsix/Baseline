{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- 1. LightGBM 최적 파라미터로 모델 학습 --------------------------\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 66228, number of negative: 190123\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 743\n",
      "[LightGBM] [Info] Number of data points in the train set: 256351, number of used features: 63\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 2\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 17 dense feature groups (4.89 MB) transferred to GPU in 0.004208 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "중요도가 낮은 피처: ['동결 배아 사용 여부', '불임 원인 - 정자 형태', '불임 원인 - 정자 면역학적 요인', '난자 해동 경과일', '불임 원인 - 자궁경부 문제', '불임 원인 - 여성 요인', '난자 채취 경과일']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import  OrdinalEncoder, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import optuna\n",
    "\n",
    "numeric_columns = [\n",
    "    \"임신 시도 또는 마지막 임신 경과 연수\",\n",
    "    \"총 생성 배아 수\",\n",
    "    \"미세주입된 난자 수\",\n",
    "    \"미세주입에서 생성된 배아 수\",\n",
    "    \"이식된 배아 수\",\n",
    "    \"미세주입 배아 이식 수\",\n",
    "    \"저장된 배아 수\",\n",
    "    \"미세주입 후 저장된 배아 수\",\n",
    "    \"해동된 배아 수\",\n",
    "    \"해동 난자 수\",\n",
    "    \"수집된 신선 난자 수\",\n",
    "    \"저장된 신선 난자 수\",\n",
    "    \"혼합된 난자 수\",\n",
    "    \"파트너 정자와 혼합된 난자 수\",\n",
    "    \"기증자 정자와 혼합된 난자 수\",\n",
    "    \"난자 채취 경과일\",\n",
    "    \"난자 해동 경과일\",\n",
    "    \"난자 혼합 경과일\",\n",
    "    \"배아 이식 경과일\",\n",
    "    \"배아 해동 경과일\"\n",
    "]\n",
    "\n",
    "categorical_columns = [\n",
    "    \"시술 시기 코드\",\n",
    "    \"시술 당시 나이\",\n",
    "    \"시술 유형\",\n",
    "    \"특정 시술 유형\",\n",
    "    \"배란 자극 여부\",\n",
    "    \"배란 유도 유형\",\n",
    "    \"단일 배아 이식 여부\",\n",
    "    \"착상 전 유전 검사 사용 여부\",\n",
    "    \"착상 전 유전 진단 사용 여부\",\n",
    "    \"남성 주 불임 원인\",\n",
    "    \"남성 부 불임 원인\",\n",
    "    \"여성 주 불임 원인\",\n",
    "    \"여성 부 불임 원인\",\n",
    "    \"부부 주 불임 원인\",\n",
    "    \"부부 부 불임 원인\",\n",
    "    \"불명확 불임 원인\",\n",
    "    \"불임 원인 - 난관 질환\",\n",
    "    \"불임 원인 - 남성 요인\",\n",
    "    \"불임 원인 - 배란 장애\",\n",
    "    \"불임 원인 - 여성 요인\",\n",
    "    \"불임 원인 - 자궁경부 문제\",\n",
    "    \"불임 원인 - 자궁내막증\",\n",
    "    \"불임 원인 - 정자 농도\",\n",
    "    \"불임 원인 - 정자 면역학적 요인\",\n",
    "    \"불임 원인 - 정자 운동성\",\n",
    "    \"불임 원인 - 정자 형태\",\n",
    "    \"배아 생성 주요 이유\",\n",
    "    \"총 시술 횟수\",\n",
    "    \"클리닉 내 총 시술 횟수\",\n",
    "    \"IVF 시술 횟수\",\n",
    "    \"DI 시술 횟수\",\n",
    "    \"총 임신 횟수\",\n",
    "    \"IVF 임신 횟수\",\n",
    "    \"DI 임신 횟수\",\n",
    "    \"총 출산 횟수\",\n",
    "    \"IVF 출산 횟수\",\n",
    "    \"DI 출산 횟수\",\n",
    "    \"난자 출처\",\n",
    "    \"정자 출처\",\n",
    "    \"난자 기증자 나이\",\n",
    "    \"정자 기증자 나이\",\n",
    "    \"동결 배아 사용 여부\",\n",
    "    \"신선 배아 사용 여부\",\n",
    "    \"기증 배아 사용 여부\",\n",
    "    \"대리모 여부\",\n",
    "    \"PGD 시술 여부\",\n",
    "    \"PGS 시술 여부\"\n",
    "]\n",
    "\n",
    "train = pd.read_csv('./data/train.csv').drop(columns=['ID'])\n",
    "test = pd.read_csv('./data/test.csv').drop(columns=['ID'])\n",
    "\n",
    "X = train.drop('임신 성공 여부', axis=1)\n",
    "y = train['임신 성공 여부']\n",
    "\n",
    "# 카테고리형 컬럼들을 문자열로 변환\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].astype(str)\n",
    "    test[col] = test[col].astype(str)\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "X_train_encoded = X.copy()\n",
    "X_train_encoded[categorical_columns] = ordinal_encoder.fit_transform(X[categorical_columns])\n",
    "\n",
    "X_test_encoded = test.copy()\n",
    "X_test_encoded[categorical_columns] = ordinal_encoder.transform(test[categorical_columns])\n",
    "\n",
    "# 수치형 컬럼들을 0으로 채움\n",
    "X_train_encoded[numeric_columns] = X_train_encoded[numeric_columns].fillna(0)\n",
    "X_test_encoded[numeric_columns] = X_test_encoded[numeric_columns].fillna(0)\n",
    "\n",
    "# **PCA**로 차원 축소 후 군집화 수행 (군집 수는 3~6 사이로 실험)\n",
    "pca = PCA(n_components=7, random_state=42)\n",
    "X_pca = pca.fit_transform(X_train_encoded[numeric_columns])\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans_clusters = kmeans.fit_predict(X_pca)\n",
    "\n",
    "# 새로운 군집 피처 추가\n",
    "X_train_encoded['cluster_group'] = kmeans_clusters\n",
    "X_test_encoded['cluster_group'] = kmeans.predict(pca.transform(X_test_encoded[numeric_columns]))\n",
    "\n",
    "# categorical_feature에 인덱스를 전달\n",
    "categorical_feature_indices = [X_train_encoded.columns.get_loc(col) for col in categorical_columns]\n",
    "\n",
    "# 스케일링 적용\n",
    "scaler = StandardScaler()\n",
    "X_train_encoded[numeric_columns] = scaler.fit_transform(X_train_encoded[numeric_columns])\n",
    "X_test_encoded[numeric_columns] = scaler.transform(X_test_encoded[numeric_columns])\n",
    "\n",
    "# # 데이터 불균형 확인 및 SMOTE + Tomek 적용\n",
    "# smt = SMOTETomek(sampling_strategy=0.5, random_state=42)\n",
    "# X_train_encoded, y_train = smt.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Feature Selection (SelectKBest)\n",
    "# selector = SelectKBest(score_func=mutual_info_classif, k=40)  # 가장 중요한 40개의 변수를 선택\n",
    "# X_train_encoded = selector.fit_transform(X_train_encoded, y)\n",
    "# X_test_encoded = selector.transform(X_test_encoded)\n",
    "\n",
    "print(\"-------------------------- 1. LightGBM 최적 파라미터로 모델 학습 --------------------------\")\n",
    "lgb_est = LGBMClassifier(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    objective='binary',\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=70,\n",
    "    max_depth=-1,\n",
    "    device='gpu',  # GPU 사용\n",
    "    gpu_platform_id=0,\n",
    "    gpu_device_id=2\n",
    ")\n",
    "\n",
    "# LightGBM 모델 학습\n",
    "lgb_est.fit(X_train_encoded, y)\n",
    "\n",
    "# LightGBM으로 Feature Importance 확인\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train_encoded.columns,\n",
    "    'importance': lgb_est.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "low_importance_features = feature_importance[feature_importance['importance'] <= 10]['feature'].tolist()\n",
    "print(\"중요도가 낮은 피처:\", low_importance_features)\n",
    "\n",
    "# 중요도가 낮은 피처 제거\n",
    "X_train_encoded = X_train_encoded.drop(columns=low_importance_features)\n",
    "X_test_encoded = X_test_encoded.drop(columns=low_importance_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def objective_lgb(trial):\n",
    "#     params = {\n",
    "#         'objective': 'binary',\n",
    "#         'n_estimators': 1000,\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "#         'num_leaves': trial.suggest_int('num_leaves', 31, 100),\n",
    "#         'max_depth': trial.suggest_int('max_depth', -1, 20),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "#         'device': 'gpu',\n",
    "#         'gpu_platform_id': 0,\n",
    "#         'gpu_device_id': 2\n",
    "#     }\n",
    "    \n",
    "#     model = LGBMClassifier(**params)\n",
    "#     model.fit(X_train_reduced, y_train)\n",
    "#     y_pred = model.predict_proba(X_val_reduced)[:, 1]\n",
    "#     return roc_auc_score(y_val, y_pred)\n",
    "\n",
    "# study_lgb = optuna.create_study(direction='maximize')\n",
    "# study_lgb.optimize(objective_lgb, n_trials=50)\n",
    "# print(\"Best parameters for LightGBM:\", study_lgb.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_xgb(trial):\n",
    "#     params = {\n",
    "#         'n_estimators': 300,\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "#         'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "#         'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "#         'tree_method': 'hist',  # 권장 방식\n",
    "#         'device': 'cuda'\n",
    "#     }\n",
    "    \n",
    "#     model = XGBClassifier(**params)\n",
    "#     model.fit(X_train_reduced, y_train)\n",
    "#     y_pred = model.predict_proba(X_val_reduced)[:, 1]\n",
    "#     return roc_auc_score(y_val, y_pred)\n",
    "\n",
    "# study_xgb = optuna.create_study(direction='maximize')\n",
    "# study_xgb.optimize(objective_xgb, n_trials=50)\n",
    "# print(\"Best parameters for XGBoost:\", study_xgb.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective_cat(trial):\n",
    "#     params = {\n",
    "#         'iterations': 1000,\n",
    "#         'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "#         'depth': trial.suggest_int('depth', 4, 12),\n",
    "#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n",
    "#         'random_seed': 42,\n",
    "#         'task_type': 'GPU',\n",
    "#         'devices': '0'\n",
    "#     }\n",
    "    \n",
    "#     model = CatBoostClassifier(**params)\n",
    "#     model.fit(X_train_reduced, y_train, verbose=0)\n",
    "#     y_pred = model.predict_proba(X_val_reduced)[:, 1]\n",
    "#     return roc_auc_score(y_val, y_pred)\n",
    "\n",
    "# study_cat = optuna.create_study(direction='maximize')\n",
    "# study_cat.optimize(objective_cat, n_trials=50)\n",
    "# print(\"Best parameters for CatBoost:\", study_cat.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-------------------------- 1. LightGBM 최적 파라미터로 모델 학습 --------------------------\")\n",
    "# Best parameters for LightGBM: {'learning_rate': 0.010682507488382762, 'num_leaves': 49, 'max_depth': 17, 'subsample': 0.8215664737448534, 'colsample_bytree': 0.8497893445033722}\n",
    "lgb_est = LGBMClassifier(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    objective='binary',\n",
    "    n_estimators=1000,\n",
    "    subsample=0.82,\n",
    "    colsample_bytree=0.85,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=49,\n",
    "    max_depth=17,\n",
    "    device='gpu',  # GPU 사용\n",
    "    gpu_platform_id=0,\n",
    "    gpu_device_id=2\n",
    ")\n",
    "\n",
    "# LightGBM 모델 학습\n",
    "lgb_est.fit(X_train_reduced, y_train)\n",
    "y_train_pred_lgb = lgb_est.predict_proba(X_val_reduced)[:, 1]\n",
    "roc_auc_lgb = roc_auc_score(y_val, y_train_pred_lgb)\n",
    "\n",
    "print(\"-------------------------- 2. XGBoost 최적 파라미터로 모델 학습 --------------------------\")\n",
    "# Best parameters for XGBoost: {'learning_rate': 0.027828379673739464, 'max_depth': 9, 'subsample': 0.8401352803766647, 'colsample_bytree': 0.817257623227272, 'gamma': 4.054300711621877}\n",
    "xgb_est = XGBClassifier(\n",
    "    eval_metric='auc',\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.027,\n",
    "    max_depth=9,\n",
    "    subsample=0.84,\n",
    "    colsample_bytree=0.82,\n",
    "    gamma=4,\n",
    "    tree_method='hist',  # 새 권장 방식\n",
    "    device='cuda'        # GPU 사용\n",
    ")\n",
    "\n",
    "# XGBoost 모델 학습\n",
    "xgb_est.fit(X_train_reduced, y_train)\n",
    "y_train_pred_xgb = xgb_est.predict_proba(X_val_reduced)[:, 1]\n",
    "roc_auc_xgb = roc_auc_score(y_val, y_train_pred_xgb)\n",
    "\n",
    "print(\"-------------------------- 3. CatBoost 최적 파라미터로 모델 학습 --------------------------\")\n",
    "# Best parameters for CatBoost: {'learning_rate': 0.03222538120048102, 'depth': 6, 'l2_leaf_reg': 1.380048205342132}\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.032,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=1.38,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    random_seed=42,\n",
    "    verbose=100,  # 경고 메시지 최소화\n",
    "    task_type='GPU',\n",
    "    devices='1'\n",
    ")\n",
    "\n",
    "# CatBoost 모델 학습\n",
    "catboost_model.fit(X_train_reduced, y_train)\n",
    "y_train_pred_cat = catboost_model.predict_proba(X_val_reduced)[:, 1]\n",
    "roc_auc_cat = roc_auc_score(y_val, y_train_pred_cat)\n",
    "\n",
    "def objective_weights(trial):\n",
    "    w1 = trial.suggest_float('w1', 0, 1)\n",
    "    w2 = trial.suggest_float('w2', 0, 1 - w1)\n",
    "    w3 = 1 - w1 - w2\n",
    "    \n",
    "    final_pred_proba = w1 * y_train_pred_lgb + w2 * y_train_pred_xgb + w3 * y_train_pred_cat\n",
    "    return roc_auc_score(y_val, final_pred_proba)\n",
    "\n",
    "study_weights = optuna.create_study(direction='maximize')\n",
    "study_weights.optimize(objective_weights, n_trials=50)\n",
    "best_weights = study_weights.best_params\n",
    "print(f\"최적 가중치: LightGBM={best_weights['w1']:.2f}, XGBoost={best_weights['w2']:.2f}, CatBoost={1 - best_weights['w1'] - best_weights['w2']:.2f}\")\n",
    "# 최적 가중치: LightGBM=0.14, XGBoost=0.31, CatBoost=0.55\n",
    "# 최적 가중치: LightGBM=0.18, XGBoost=0.34, CatBoost=0.48\n",
    "# 최적 가중치: LightGBM=0.15, XGBoost=0.32, CatBoost=0.53\n",
    "# 최적 가중치: LightGBM=0.14, XGBoost=0.34, CatBoost=0.52\n",
    "final_pred_proba = 0.15 * y_train_pred_lgb + 0.33 * y_train_pred_xgb + 0.52 * y_train_pred_cat\n",
    "roc_auc_final = roc_auc_score(y_val, final_pred_proba)\n",
    "\n",
    "print(f\"LightGBM 모델의 ROC-AUC 점수: {roc_auc_lgb:.4f}\")\n",
    "print(f\"XGBoost 모델의 ROC-AUC 점수: {roc_auc_xgb:.4f}\")\n",
    "print(f\"CatBoost 모델의 ROC-AUC 점수: {roc_auc_cat:.4f}\")\n",
    "print(f\"Final 모델의 ROC-AUC 점수: {roc_auc_final:.4f}\")\n",
    "# print(f\"StackingClassifier 모델의 ROC-AUC 점수: {ROC_AUC_stacking:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM 모델의 ROC-AUC 점수: 0.7359\n",
    "# XGBoost 모델의 ROC-AUC 점수: 0.7308\n",
    "# CatBoost 모델의 ROC-AUC 점수: 0.7354\n",
    "# StackingClassifier 모델의 ROC-AUC 점수: 0.7201\n",
    "\n",
    "# SMOTETomek 적용 0.8\n",
    "# LightGBM 모델의 ROC-AUC 점수: 0.7364 \n",
    "# XGBoost 모델의 ROC-AUC 점수: 0.7320 ++++++++++++++++++++++++++++++++ 최고\n",
    "# CatBoost 모델의 ROC-AUC 점수: 0.7361\n",
    "\n",
    "# SMOTETomek 적용 0.5\n",
    "# LightGBM 모델의 ROC-AUC 점수: 0.7366 ++++++++++++++++++++++++++++++++ 최고\n",
    "# XGBoost 모델의 ROC-AUC 점수: 0.7317\n",
    "# CatBoost 모델의 ROC-AUC 점수: 0.7363\n",
    "\n",
    "# # 미적용\n",
    "# LightGBM 모델의 ROC-AUC 점수: 0.7364\n",
    "# XGBoost 모델의 ROC-AUC 점수: 0.7276\n",
    "# CatBoost 모델의 ROC-AUC 점수: 0.7368\n",
    "# Final 모델의 ROC-AUC 점수: 0.7359\n",
    "\n",
    "# Q1-3 제거\n",
    "# LightGBM 모델의 ROC-AUC 점수: 0.7364\n",
    "# XGBoost 모델의 ROC-AUC 점수: 0.7292\n",
    "# CatBoost 모델의 ROC-AUC 점수: 0.7369 ++++++++++++++++++++++++++++++++ 최고\n",
    "# Final 모델의 ROC-AUC 점수: 0.7361\n",
    "\n",
    "# # Feature engnieering 적용\n",
    "# LightGBM 모델의 ROC-AUC 점수: 0.7362\n",
    "# XGBoost 모델의 ROC-AUC 점수: 0.7299\n",
    "# CatBoost 모델의 ROC-AUC 점수: 0.7368\n",
    "# Final 모델의 ROC-AUC 점수: 0.7363\n",
    "\n",
    "# KNMeans 적용 군집\n",
    "# LightGBM 모델의 ROC-AUC 점수: 0.7363\n",
    "# XGBoost 모델의 ROC-AUC 점수: 0.7301\n",
    "# CatBoost 모델의 ROC-AUC 점수: 0.7368\n",
    "# Final 모델의 ROC-AUC 점수: 0.7364 \n",
    "\n",
    "# feature importance 상위 30개 변수만 사용\n",
    "# LightGBM 모델의 ROC-AUC 점수: 0.7356\n",
    "# XGBoost 모델의 ROC-AUC 점수: 0.7278\n",
    "# CatBoost 모델의 ROC-AUC 점수: 0.7364\n",
    "# Final 모델의 ROC-AUC 점수: 0.7355\n",
    "\n",
    "# feature importance ['불임 원인 - 정자 형태', '불임 원인 - 정자 면역학적 요인', '난자 해동 경과일', '불임 원인 - 자궁경부 문제', '불임 원인 - 여성 요인', '난자 채취 경과일'] 제거\n",
    "# LightGBM 모델의 ROC-AUC 점수: 0.7363\n",
    "# XGBoost 모델의 ROC-AUC 점수: 0.7299\n",
    "# CatBoost 모델의 ROC-AUC 점수: 0.7369\n",
    "# Final 모델의 ROC-AUC 점수: 0.7364  \n",
    "\n",
    "# kmeans 5개 군집 PCA\n",
    "# LightGBM 모델의 ROC-AUC 점수: 0.7365\n",
    "# XGBoost 모델의 ROC-AUC 점수: 0.7297\n",
    "# CatBoost 모델의 ROC-AUC 점수: 0.7368\n",
    "# Final 모델의 ROC-AUC 점수: 0.7369\n",
    "\n",
    "# hyperparameter tuning\n",
    "# LightGBM 모델의 ROC-AUC 점수: 0.7369\n",
    "# XGBoost 모델의 ROC-AUC 점수: 0.7368\n",
    "# CatBoost 모델의 ROC-AUC 점수: 0.7371\n",
    "# Final 모델의 ROC-AUC 점수: 0.7375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------- 1. LightGBM 최적 파라미터로 모델 학습 --------------------------\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of positive: 66228, number of negative: 190123\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 738\n",
      "[LightGBM] [Info] Number of data points in the train set: 256351, number of used features: 61\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 2\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3090, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 64 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 17 dense feature groups (4.89 MB) transferred to GPU in 0.004743 secs. 1 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "-------------------------- 2. XGBoost 최적 파라미터로 모델 학습 --------------------------\n",
      "-------------------------- 3. CatBoost 최적 파라미터로 모델 학습 --------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 4.71ms\tremaining: 4.71s\n",
      "100:\ttotal: 374ms\tremaining: 3.33s\n",
      "200:\ttotal: 726ms\tremaining: 2.88s\n",
      "300:\ttotal: 1.08s\tremaining: 2.5s\n",
      "400:\ttotal: 1.43s\tremaining: 2.13s\n",
      "500:\ttotal: 1.78s\tremaining: 1.77s\n",
      "600:\ttotal: 2.13s\tremaining: 1.42s\n",
      "700:\ttotal: 2.48s\tremaining: 1.06s\n",
      "800:\ttotal: 2.83s\tremaining: 704ms\n",
      "900:\ttotal: 3.18s\tremaining: 350ms\n",
      "999:\ttotal: 3.53s\tremaining: 0us\n",
      "최종 제출 파일 'final_ensemble_submit.csv' 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------- 1. LightGBM 최적 파라미터로 모델 학습 --------------------------\")\n",
    "# Best parameters for LightGBM: {'learning_rate': 0.010682507488382762, 'num_leaves': 49, 'max_depth': 17, 'subsample': 0.8215664737448534, 'colsample_bytree': 0.8497893445033722}\n",
    "lgb_est = LGBMClassifier(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    objective='binary',\n",
    "    n_estimators=1000,\n",
    "    subsample=0.82,\n",
    "    colsample_bytree=0.85,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=49,\n",
    "    max_depth=17,\n",
    "    device='gpu',  # GPU 사용\n",
    "    gpu_platform_id=0,\n",
    "    gpu_device_id=2\n",
    ")\n",
    "\n",
    "# LightGBM 모델 학습\n",
    "lgb_est.fit(X_train_encoded, y)\n",
    "y_train_pred_lgb = lgb_est.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "print(\"-------------------------- 2. XGBoost 최적 파라미터로 모델 학습 --------------------------\")\n",
    "# Best parameters for XGBoost: {'learning_rate': 0.027828379673739464, 'max_depth': 9, 'subsample': 0.8401352803766647, 'colsample_bytree': 0.817257623227272, 'gamma': 4.054300711621877}\n",
    "xgb_est = XGBClassifier(\n",
    "    eval_metric='auc',\n",
    "    random_state=42,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.027,\n",
    "    max_depth=9,\n",
    "    subsample=0.84,\n",
    "    colsample_bytree=0.82,\n",
    "    gamma=4,\n",
    "    tree_method='hist',  # 새 권장 방식\n",
    "    device='cuda'        # GPU 사용\n",
    ")\n",
    "\n",
    "# XGBoost 모델 학습\n",
    "xgb_est.fit(X_train_encoded, y)\n",
    "y_train_pred_xgb = xgb_est.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "print(\"-------------------------- 3. CatBoost 최적 파라미터로 모델 학습 --------------------------\")\n",
    "# Best parameters for CatBoost: {'learning_rate': 0.03222538120048102, 'depth': 6, 'l2_leaf_reg': 1.380048205342132}\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.032,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=1.38,\n",
    "    loss_function='Logloss',\n",
    "    eval_metric='AUC',\n",
    "    random_seed=42,\n",
    "    verbose=100,  # 경고 메시지 최소화\n",
    "    task_type='GPU',\n",
    "    devices='1'\n",
    ")\n",
    "\n",
    "# CatBoost 모델 학습\n",
    "catboost_model.fit(X_train_encoded, y)\n",
    "y_train_pred_cat = catboost_model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "final_pred_proba = 0.1 * y_train_pred_lgb + 0.1 * y_train_pred_xgb + 0.8 * y_train_pred_cat\n",
    "\n",
    "# 제출 파일 생성\n",
    "sample_submission = pd.read_csv('./data/sample_submission.csv')\n",
    "sample_submission['probability'] = final_pred_proba\n",
    "sample_submission.to_csv('./submit/final_ensemble_submit.csv', index=False)\n",
    "print(\"최종 제출 파일 'final_ensemble_submit.csv' 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 교차 검증 설정\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# cv_scores_lgb = []\n",
    "# cv_scores_xgb = []\n",
    "# cv_scores_stack = []\n",
    "# cv_scores_catb = []\n",
    "\n",
    "# # 교차 검증을 통한 ROC-AUC 비교\n",
    "# for train_idx, val_idx in cv.split(X_train_encoded, y):\n",
    "#     X_cv_train, X_cv_val = X_train_encoded.iloc[train_idx], X_train_encoded.iloc[val_idx]\n",
    "#     y_cv_train, y_cv_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "#     # LightGBM 예측\n",
    "#     lgb_est.fit(X_cv_train, y_cv_train)\n",
    "#     lgb_proba = lgb_est.predict_proba(X_cv_val)[:, 1]\n",
    "#     auc_lgb = roc_auc_score(y_cv_val, lgb_proba)\n",
    "#     cv_scores_lgb.append(auc_lgb)\n",
    "    \n",
    "#     # XGBoost 예측\n",
    "#     xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "#     xgb_model.fit(X_cv_train, y_cv_train)\n",
    "#     xgb_proba = xgb_model.predict_proba(X_cv_val)[:, 1]\n",
    "#     auc_xgb = roc_auc_score(y_cv_val, xgb_proba)\n",
    "#     cv_scores_xgb.append(auc_xgb)\n",
    "\n",
    "#     # catboost 예측\n",
    "#     catboost_model.fit(X_cv_train, y_cv_train)\n",
    "#     cat_proba = catboost_model.predict_proba(X_cv_val)[:, 1]\n",
    "#     auc_catb = roc_auc_score(y_cv_val, cat_proba)\n",
    "#     cv_scores_catb.append(auc_catb)\n",
    "    \n",
    "#     # Stacking 앙상블 예측\n",
    "#     stacking_clf.fit(X_cv_train, y_cv_train)\n",
    "#     stack_proba = stacking_clf.predict_proba(X_cv_val)[:, 1]\n",
    "#     auc_stack = roc_auc_score(y_cv_val, stack_proba)\n",
    "#     cv_scores_stack.append(auc_stack)\n",
    "\n",
    "# # 평균 ROC-AUC 출력\n",
    "# print(\"------------- 교차 검증 평균 ROC-AUC -------------\")\n",
    "# print(f\"LightGBM: {np.mean(cv_scores_lgb):.4f}\")\n",
    "# print(f\"XGBoost: {np.mean(cv_scores_xgb):.4f}\")\n",
    "# print(f\"Stacking Ensemble: {np.mean(cv_scores_stack):.4f}\")\n",
    "# print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
